{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUm8bbQbDUJISXTbU2KJ5A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Soft Computing Experiment 6\n","Perceptron Network (XOR)"],"metadata":{"id":"fPKpmY2xlLzW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyzAyuNclFeM","executionInfo":{"status":"ok","timestamp":1724867451528,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Rohit Deshpande","userId":"11530750372540555607"}},"outputId":"74efeb97-fe6c-47c8-aad3-c431188d389f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final predicted output after training:\n","[[0.0189]\n"," [0.9837]\n"," [0.9837]\n"," [0.0169]]\n","Testing the MLP on XOR inputs:\n","Input: [0 0], Predicted Output: [[0.0189]], Expected Output: [0]\n","Input: [0 1], Predicted Output: [[0.9837]], Expected Output: [1]\n","Input: [1 0], Predicted Output: [[0.9837]], Expected Output: [1]\n","Input: [1 1], Predicted Output: [[0.0169]], Expected Output: [0]\n"]}],"source":["# @title MLP Perceptron Learning\n","import numpy as np\n","\n","# Sigmoid activation function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Derivative of sigmoid function\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Training data for XOR\n","X = np.array([[0, 0],\n","              [0, 1],\n","              [1, 0],\n","              [1, 1]])\n","\n","# XOR outputs\n","y = np.array([[0], [1], [1], [0]])\n","\n","# Set seed for reproducibility\n","np.random.seed(42)\n","\n","# Initialize weights and biases for the MLP\n","inputLayer_neurons = 2    # Number of input neurons (x1 and x2)\n","hiddenLayer_neurons = 2   # Number of neurons in the hidden layer\n","output_neurons = 1        # Number of output neurons (1 for XOR output)\n","\n","# Random initialization of weights and biases\n","hidden_weights = np.random.uniform(size=(inputLayer_neurons, hiddenLayer_neurons))\n","hidden_bias = np.random.uniform(size=(1, hiddenLayer_neurons))\n","\n","output_weights = np.random.uniform(size=(hiddenLayer_neurons, output_neurons))\n","output_bias = np.random.uniform(size=(1, output_neurons))\n","\n","# Learning rate\n","learning_rate = 0.5\n","\n","# Training loop for the MLP\n","epochs = 10000\n","for epoch in range(epochs):\n","    # Forward propagation\n","\n","    # Compute hidden layer activation\n","    hidden_layer_activation = np.dot(X, hidden_weights) + hidden_bias\n","    hidden_layer_output = sigmoid(hidden_layer_activation)\n","\n","    # Compute output layer activation\n","    output_layer_activation = np.dot(hidden_layer_output, output_weights) + output_bias\n","    predicted_output = sigmoid(output_layer_activation)\n","\n","    # Backpropagation\n","\n","    # Calculate the error\n","    error = y - predicted_output\n","\n","    # Derivative of output (error * derivative of sigmoid)\n","    d_predicted_output = error * sigmoid_derivative(predicted_output)\n","\n","    # Error of hidden layer\n","    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n","\n","    # Derivative of hidden layer output\n","    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n","\n","    # Update the weights and biases\n","    output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n","    output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n","    hidden_weights += X.T.dot(d_hidden_layer) * learning_rate\n","    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n","\n","# Print the final predicted output after training\n","print(\"Final predicted output after training:\")\n","print(np.round(predicted_output, 4))\n","\n","# Testing the MLP on XOR inputs\n","print(\"Testing the MLP on XOR inputs:\")\n","for i in range(len(X)):\n","    hidden_layer_activation = np.dot(X[i], hidden_weights) + hidden_bias\n","    hidden_layer_output = sigmoid(hidden_layer_activation)\n","    output_layer_activation = np.dot(hidden_layer_output, output_weights) + output_bias\n","    predicted_output = sigmoid(output_layer_activation)\n","\n","    print(f\"Input: {X[i]}, Predicted Output: {np.round(predicted_output, 4)}, Expected Output: {y[i]}\")"]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","# Create the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(2, activation='sigmoid', input_shape=(2,)),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Create the dataset\n","X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n","y = [0, 1, 1, 0]\n","\n","# Convert X and y to NumPy arrays\n","X = np.array(X)\n","y = np.array(y)\n","\n","# Train the model\n","model.fit(X, y, epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fuFQWuKry6x8","executionInfo":{"status":"ok","timestamp":1725281716976,"user_tz":-330,"elapsed":1438,"user":{"displayName":"Rohit Deshpande","userId":"11530750372540555607"}},"outputId":"3d767f9b-e90f-4bb0-c6d3-b1b2021370c5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896ms/step - accuracy: 0.5000 - loss: 0.7299\n","Epoch 2/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.7296\n","Epoch 3/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7293\n","Epoch 4/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7289\n","Epoch 5/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.7286\n","Epoch 6/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7283\n","Epoch 7/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7280\n","Epoch 8/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7277\n","Epoch 9/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7273\n","Epoch 10/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7270\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x78505659fdf0>"]},"metadata":{},"execution_count":4}]}]}