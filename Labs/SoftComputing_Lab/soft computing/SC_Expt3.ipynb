{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtrfj8p34LIjeP8GRLuz+y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Hebbian Learning Rule\n","1. Binary Activation Function"],"metadata":{"id":"9sRmdWSyqb4j"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LToPtVOsp22m","executionInfo":{"status":"ok","timestamp":1723619861001,"user_tz":-330,"elapsed":454,"user":{"displayName":"Rohit Deshpande","userId":"11530750372540555607"}},"outputId":"a791fe00-44ce-40dc-9e65-42a65395dc28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Weights (Binary): [ 1.  -1.   0.   0.5]\n","Net input for X1: 3.0\n","Activation for X1: 1\n","Updated Weights after X1: [ 2.  -3.   1.5  0.5]\n","Net input for X2: -0.25\n","Activation for X2: -1\n","Updated Weights after X2: [ 1.  -2.5  3.5  2. ]\n","Net input for X3: -3.0\n","Activation for X3: -1\n","Updated Weights after X3: [ 1.  -3.5  4.5  0.5]\n","Initial Weights:  [ 1.  -1.   0.   0.5]\n","Weights after Hebbian Learning with Binary Activation:  [ 1.  -3.5  4.5  0.5]\n"]}],"source":["import numpy as np\n","\n","X1 = np.array([1, -2, 1.5, 0])\n","X2 = np.array([1, -0.5, -2, -1.5])\n","X3 = np.array([0, 1, -1, 1.5])\n","\n","w1 = np.array([1, -1, 0, 0.5])\n","\n","# Binary Activation Function\n","def binary_activation(x):\n","    return np.where(x > 0, 1, np.where(x < 0, -1, 0))\n","\n","# Hebbian Learning using Binary Activation Function\n","def hebbian_learning_binary(weights, inputs):\n","    print(\"Initial Weights (Binary):\", weights)\n","    for i, x in enumerate(inputs):\n","        net = np.dot(weights, x)\n","        act = binary_activation(net)\n","        print(f\"Net input for X{i+1}:\", net)\n","        print(f\"Activation for X{i+1}:\", act)\n","        weights += act * x\n","        print(f\"Updated Weights after X{i+1}:\", weights)\n","    return weights\n","\n","weights_binary = w1.copy()\n","weights_binary = hebbian_learning_binary(weights_binary, [X1, X2, X3])\n","\n","print(\"Initial Weights: \", w1)\n","print(\"Weights after Hebbian Learning with Binary Activation: \", weights_binary)\n"]},{"cell_type":"markdown","source":["2. Bipolar Activation Function"],"metadata":{"id":"ZmrufvE_qnty"}},{"cell_type":"code","source":["import numpy as np\n","\n","X1 = np.array([1, -2, 1.5, 0])\n","X2 = np.array([1, -0.5, -2, -1.5])\n","X3 = np.array([0, 1, -1, 1.5])\n","\n","w1 = np.array([1, -1, 0, 0.5])\n","\n","#Bipolar Activation Function\n","def bipolar_activation(x):\n","    return np.where(x > 0, 1, -1)\n","\n","# Hebbian Learning using Bipolar Activation Function\n","def hebbian_learning_bipolar(weights, inputs):\n","    print(\"Initial Weights (Bipolar):\", weights)\n","    for i, x in enumerate(inputs):\n","        net = np.dot(weights, x)\n","        act = bipolar_activation(net)\n","        print(f\"Net input for X{i+1}:\", net)\n","        print(f\"Activation for X{i+1}:\", act)\n","        weights += act * x\n","        print(f\"Updated Weights after X{i+1}:\", weights)\n","    return weights\n","\n","weights_bipolar = w1.copy()\n","weights_bipolar = hebbian_learning_bipolar(weights_bipolar, [X1, X2, X3])\n","\n","print(\"Initial Weights: \", w1)\n","print(\"Weights after Hebbian Learning with Bipolar Activation: \", weights_bipolar)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-J-W8DXqrMi","executionInfo":{"status":"ok","timestamp":1723619853020,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rohit Deshpande","userId":"11530750372540555607"}},"outputId":"1609793b-1b1e-42f7-a751-04812cacee56"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Weights (Bipolar): [ 1.  -1.   0.   0.5]\n","Net input for X1: 3.0\n","Activation for X1: 1\n","Updated Weights after X1: [ 2.  -3.   1.5  0.5]\n","Net input for X2: -0.25\n","Activation for X2: -1\n","Updated Weights after X2: [ 1.  -2.5  3.5  2. ]\n","Net input for X3: -3.0\n","Activation for X3: -1\n","Updated Weights after X3: [ 1.  -3.5  4.5  0.5]\n","Initial Weights:  [ 1.  -1.   0.   0.5]\n","Weights after Hebbian Learning with Bipolar Activation:  [ 1.  -3.5  4.5  0.5]\n"]}]}]}